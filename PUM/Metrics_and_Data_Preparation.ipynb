{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balanced accuracy = (sensitivity + specificity) / 2 # balansuje niezrownowazenie zbioru jesli np model wykrywa lepiej pozytywy lub negatywy to wlasnie balanced accuracy wskaze ktory sobie lepiej radzi\n",
    "\n",
    "adjusted balanced accuracy = sensitivity + specificity - 1 # jest znane jako statystyka J (J score) wynosi zero dla losowej skutecznosci jesli powyzej zera to wiemy ze dziala lepiej niz losowe \n",
    "\n",
    "F1 Score = 2 *  (Precision * sensitivity)/(precision + sensitivity)\n",
    "\n",
    "false match rate = FP / (FP+TN)\n",
    "\n",
    "false non match rate = FN / (TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowywanie danych\n",
    "\n",
    "Każde dane mają szum. Brakujące dane. Literówki w zmiennych kategorycznych, Przygotowanie danych to EDA. Jesliw artosc ciagla to rysuje histogram. Jesli wartosc kategoryczna ile danej kategorii jest. Niezbilansowany zbior danych. Czy ciągle czy dyskretne. Krok eksploracyjny czyli cecha po cesze. Czyszczenie polega są brakujące wartości. Dwie możliwości:  \n",
    "* usunać kazda probke bez danych ale bardzo kosztowne, danych nadzorowanych nie jest duzo\n",
    "* Imputujemy dane: zakładamy ze to co brakuje było typowe: dla danych ciągłych to będzie średnia, dla dyskretnych mediana,\n",
    "dla zmiennych kateogrycznych to bierzemy dominante. Zmienne sekwencyjne to robimy interpolacje. Sekwencyjne to np. czas czyli co minute mierzy. Nigdy przenigdy nie wolno uzupelnic etykiety. Jesli brakuje etykiety to albo usuwamy albo pierwotny zbior danych i tam sprawdzamy czy mogla tam nalezec. Zeby moc stwierdzic co jest typowe. jesli w danej kolumnie jest wiecej niz 30% brakujacych dane to juz nie mozemy imputowac, wiec po prostu usuwamy. Imputowac mozemy tylko szum wiec uzupelniamy te braki. Mamy dwa typy brakujacych dane:  \n",
    "* MCAR (Missing Completely At Random)\n",
    "* MNAR (Missing Not at random)\n",
    "\n",
    "Wtedy i tylko wtedy imputujemy kiedy missing completely at random czyli brak jest nie zwiazany czyli np. uszkodzenie czujnika\n",
    "\n",
    "Ale missing not at random to np. wyjscie poza skale czujnika \n",
    "\n",
    "CGM - do pomiaru glukozy urzadzenia \n",
    "\n",
    "Jezeli dwie trzy kolejne probki ktore brakuja wiec beda typu MCAR bo np. ktos machal rekoma, ale bedzie ten sam typ braku ale pacjent w tym czasie cwiczyl czyli wysoko meczace wiec normalne ze spada glukoza wiec to bedzie typ MNAR\n",
    "Wiec trzeba rozumiec jak powstaly te dane \n",
    "\n",
    "Mamy tez czasami problem z danymi ktorymi wystają. Są to modele numeryczne wiec nie lubia one szerokich zakresow danych. Np od 0.001 do 10000, nie bedzie sie dobrze zachowywal numerycznie klasyfikator np liniowy bo mnozenie takich danych. \n",
    "\n",
    "Jezeli mamy zbior danych i 99% procent tych danych -4,4 ale 0,5% -1000 i 0,5% 1000 to znormalizowanie takich danych spowoduje ze te dane -4,4 zostana zgniecione do bardzo malego zakresu. Dlatego chcemy obciac wartosci wystajace to nie znaczy je usunac tylko obciąć czyli jesli wartosci odstajace zaczynaja sie na progu 1000 i beda kolejne np 1001, to zmieniamy jej wartosc na 1000 w numpy to sie robi numpy. clip. Jak wybieramy wartosci odstajace te ktore odstaja o wiecej niz 1,5 wartosci miedzycwiartkowego IQR - cos w stylu Intern Quantite Rate (ale sprawdz nazwe!!)\n",
    "\n",
    "Jak jest wykres box\n",
    "\n",
    "Q1 mediana dolnej połowy zbioru\n",
    "Q2 jest mediana calego zbioru\n",
    "Q3 mediana gornej polowy zbioru \n",
    "\n",
    "IQR = odleglosc miedzy Q1 a Q3 \n",
    "\n",
    "Wartosci sa odstaje gdy sa mniejsze od Q1-1,5 IQR oraz gdy są wieksze niz Q3 + 1,5 IQR i to wlasnie to obcinanie danych \n",
    "nazywa sie winsoryzacja i robie do na kazdej cesze\n",
    "\n",
    "Ale te ogony moga byc bardzo dlugie. Wiec wtedy operujemy na percentylach czyli docianmy 5% gornych i dolnych wartosci. Sa prostsze w obsludze ale nie dobieraja tych progow jak w IQR. PErcentyle to szybko i byle jak wiec lepiej winsoryzacja. \n",
    "\n",
    "Po co winsoryzacja?  \n",
    "\n",
    "Jest zwykle krokiem PRZED **normalizacja**\n",
    "\n",
    "X' = (X - Xmin)/(Xmax - Xmin)\n",
    "\n",
    "jak chcemy do zakresu to robimy \n",
    "\n",
    "X' = a + (X - Xmin)(b-a)/(Xmax - Xmin)\n",
    "\n",
    "**Standaryzacja** \n",
    "X' = X - srednia / rozklad normalny \n",
    "\n",
    "**Wariancja**\n",
    "Wariancja = Sigma^2 = 1/n z sumy od i do N z (Xi -srednia) ^2\n",
    "\n",
    "Gdzie mozemy uzyc do standaryzacja tam gdzie przypomina rozklad normalny, tam gdzie jendomodowy rozklad normalny\n",
    "\n",
    "Poetykietowane dane są drogie \n",
    "\n",
    "Tam gdzie nie bedzie rozkladu normalnego czyli tam gdzie jest wielomodowy wiec wielomodowego nie mozna standaryzowac takiego. Standardowy rozklad normalny jest w zakresie 0,1 ale trzeba byc bardzo stroznym bo bedzeimy podawac, srednia i wariancje lub srednia i odchylenie standarodwe. Zazwyczaj bedzie to odchylenie standardowe. \n",
    "\n",
    "normalizujemy kazda z cech. \n",
    "\n",
    "Normalizowac mozemy zawsze a standaryzowac tylko wtedy gdy przypomina to rozklad normalny. \n",
    "\n",
    "Dzielimy zbior na treningowy i testowy. 80 % trening i 20% test. Ale unikamy information leakage - wyciek danych\n",
    "Jak dochodzi do wycieku danych jak najpierw znormalizujemy  najpierw i potem podzielimy na zbiory to juz testowy ma informacje o tym od jakiego zakresu zostal sprowadzony treningowy.  \n",
    "\n",
    "Wiec najpierw dzielimy na zbiory potem normalizujemy \n",
    "Winsoryzacja, na podstawie zbioru treningowego Q1 Q2 Q3 i docinamy tylko nie docianmy testowego tymi programi \n",
    "\n",
    "Zbior testowy ma prawo wystawc ponad zbior treningowy\n",
    "Metryki liczymy na obu zbiorach \n",
    "\n",
    "Jak chcemy dostroić hiperaparamtetry w sieci \n",
    "to treningowy dzielimy na 80% treningowy i 20% walidacyjny i to na 20% dostrajamy wlasnie parametry. Wycieki sa problemy bo dowiadujemy sie ze jest problem troche za pozno, jak juz sie jest na produkcji. \n",
    "\n",
    "Reprezentowanie etykiet a dokladniej zmiennych kategorycznych:\n",
    "* one hot - jeden bit jest hot, ale im wiecej kategorii tym jest juz gorzej. Wiec jest jakas ilosc zdrowo-rozsadkowa do ktorej stosujemy one hot\n",
    "* jesli zmienne sa uszeregowane to mozemy je zamienic na wartosci: maly, sredni, duzy ale jak mam pies, kot, koń to juz nie da rady tym sposobem\n",
    "* embedding polega na tym ze zamieniamy na wektor o stalej szerokosci, probuje on zachowac podobienstaw tych kateogrii, sa one stosowane w NLP (natural language processing) \n",
    "e(\"king\") - e(\"man\") + e(\"woman\") ~= e(\"queen\")\n",
    "\n",
    "Selekcja cech: do tego sluzy kryterium Fishera\n",
    "F = abs(odleglosc pomiedzy srednimi dwoch klas) / (odchylenie standardowe cechy A +  odchylenie standardowe klasy B)\n",
    "\n",
    "F = abs(odleglosc pomiedzy srednimi dwoch klas) / wyznacznik ( macierzy kowariancji cechy A + macierz kowariancji cechy B) im wyzsza warrtosc tym lepiej bo skupione ale oddalone od siebie. Mamy np 1000 cech a chcemy 20 najlepszych to kombinacji \n",
    "\n",
    "1000! / (20! * 980!) = 10^41 = 0.1 Septyliona kombinacji jesli 1000 kombinacji na sek sprawdzimy to potrwa to bedzie 10 ^38 czyli 0.1 Sekstryliarda sekund ~ 10^31 lat = 10 kwintylionow lat ~ 7,4 * 10^20 * TH ~ 10^21 TH czasu Habbla czyli czas wszechswiata \n",
    "\n",
    "Sekwencyjne kryterium Fishera najpierw wybieramy piersza najlepsza ceche i potem pierwszej dokaldamy druga najlepsza ceche i trzecia najlepsze ceche bo w kazdym kroku dokladamy jedna ceche. \n",
    "\n",
    "Mozna zrobic rekursywna eliminacje cech: model uczymy na wszystkich cechach a potem ucze na wszystkich z wyjatkiem jednej i biore ten model co mial najnizsza strate. \n",
    "\n",
    "Jak recznie wybrac cechy: licze macierz korelacji dla cech i etykiet, wybieramy wartosci ktore sa silnie skorelowane czyli blisko -1 lub 1. Jesli wybierzemy takie to sprawdzamy czy inne cechy w stosunku do tych co wybralismy maja podobne wartosci to znaczy ze beda niesc ze soba te same informacje ale. Selekcja cech moze byc przed normalizacja i winsoryzacja. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
