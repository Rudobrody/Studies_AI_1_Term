{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4c249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36cc19a7",
   "metadata": {},
   "source": [
    "to robimy na dzini tamto na enttropi ale ot nei znaczy ze entropia dziala tylko na drzewie. Nalezy pamietac o buggingu i komitet drzew decyzyjnych nie jest lasem losowym. Wielkosc lasu oparta o blad OOB. Wiec musi byc losowy wybor cech na wezlach i w ten sposob jest las losowy. Sa nadal dwie klasy bo sa klasyfikatory binarne ale maja iles ksiezycy na klase tym sa wielomodowe. Jesli braki to zuuzpelnic. PRzy lasie nie trzeba normalizowac. Na 5 jak wielkosc lasu wplywa na szybkosc inferencji scikit learn i wlasnej implementacji. To trzeba zrobic kilka razy bo windows defender itp. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f215c7",
   "metadata": {},
   "source": [
    "Wspolczynnik Gini Impurity Index mierzy prawdopodobienstwo blednej decyzji. \n",
    "\n",
    "Czyli prawdopodobienstwo etykiety * prawdopobienstwo pomylki czyli szansa na inn etykiety. \n",
    "\n",
    "Czyli suma po i: p_i*(1-p_i) to inaczej po przeksztalceniu i podniesieniu do kwadratu mamy ze to jest 1-suma_i p_i^2\n",
    "\n",
    "Jesli mamy zdarzenie pewne to gini wtedy wynosi 0. \n",
    "\n",
    "G_max odnosi sie do liczby klas wiec jesli mamy 2 kklasy to max wspl Gini wyniesie 0,5.\n",
    "\n",
    "Redukcja Gini \n",
    "\n",
    "bias vs variance tradeoff. Czyli albo agresywnie dopasowany i ingornuje ogół albo ogolnie dopasoany z wieksza wariancja ale mniejszym biasem. Komitet pozwala na redukcje wariancji, nie zwieksza on biasu. Przy glosowaniu wariancje sie znosza. Kazdy model w komitecie nalezy uczyć na osobnych podzbiorach. Boosting -- kolejne modele uczymy na zbiorze wazonym błędem poprzednich. Blad w ktoryms momencie bedzie tak niski ze nie ma sensu rozwijac. \n",
    "Druga metoda to bagging lub inaczej bootstrap aggregating. W kroku pierwszym losujemy ze zwracaniem!!!! n probek z N elementowego zbioru i robimy tak dla kazdego z B podzbiorow. Dla kazdego worka wrzucamy n probek i losujemy. Krok drugi dla kazdego z B zbiorow uczymy osobny model. Mechanika ostatniej predykcji jesli to jest regresja to f(x)= 1/ liczba podzbiorow z sumy tych zbiorow f_b(x)\n",
    "jesli to klasyfikacja a nie regresja to po prostu bierzemy dominante. Bład OOB (out of bag) kazdemu naucznonemu modelowi, dopelnieniem to co nie weszlo do worka jest zbior walidacyjny wiec kazdy ma swoj zbior walidacyjny. Ten blad sie wyplaszczy dla jakiejs liczby modeli. To moze byc komitet SVM, regresjii logistycznej i mozemy miec tez komitet drzew. Okazuje sie ze komitet mocnych modeli jest problematyczny, boosting jeszcze daje rade ale szybko dokladnie modelu malo daje a wariancja sie malo znosi bo niewielka liczba modeli. zamiast budowac komitet z ekspertow budujemy z idiotow. Przy sieciach ciezko stwierdzic czy slaby czy mocny. Przy drzewie mozemy robic 2 rzeczy dosc bezkarnie. Jak zrobic kiepskie drzewo? Drzewo losowowe ma ograniczoną glebokosc np. 1 lub 2. A wewnatrz drzewa dla kazdego węzła losujemy dopuszczalne cechy, tj dla klasyfikacji pierwiastek z ilosci cech, do regresji losujemy D/3. Po prostu taka wartosc. Drzewa decyzyjne sa zbyt mocne zeby z nich robic las losowy. Zanim zrobi podzial sprawdza z czzego moze go zrobic, bo zasada dzialania ta sama. Dla kazdego drzewa pilnowac na czym uczylismy drzewa. Latwiej zazwyczaj trzymac probki walidacyjne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc1e60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e09dc694",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
